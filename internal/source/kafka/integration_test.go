// Copyright 2024 The Cockroach Authors
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
//     http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
// SPDX-License-Identifier: Apache-2.0

package kafka

import (
	"context"
	"database/sql"
	"encoding/json"
	"fmt"
	"hash/fnv"
	"math/big"
	"math/rand/v2"
	"testing"
	"time"

	"github.com/IBM/sarama"
	"github.com/cockroachdb/field-eng-powertools/notify"
	"github.com/cockroachdb/field-eng-powertools/stopper"
	"github.com/cockroachdb/replicator/internal/conveyor"
	"github.com/cockroachdb/replicator/internal/script"
	"github.com/cockroachdb/replicator/internal/sequencer"
	"github.com/cockroachdb/replicator/internal/sinkprod"
	"github.com/cockroachdb/replicator/internal/sinktest"
	"github.com/cockroachdb/replicator/internal/sinktest/all"
	"github.com/cockroachdb/replicator/internal/sinktest/base"
	"github.com/cockroachdb/replicator/internal/sinktest/scripttest"
	"github.com/cockroachdb/replicator/internal/types"
	"github.com/cockroachdb/replicator/internal/util/hlc"
	"github.com/cockroachdb/replicator/internal/util/ident"
	"github.com/pkg/errors"
	"github.com/prometheus/client_golang/prometheus"
	log "github.com/sirupsen/logrus"
	"github.com/stretchr/testify/assert"
	"github.com/stretchr/testify/require"
)

type fixtureConfig struct {
	chaos     bool
	script    bool
	immediate bool
}

const (
	broker        = "localhost:29092"
	maxBatchSize  = 10
	maxIterations = 25
	numPartitions = 5
)

// TestMain verifies that we can run the integration test for Kafka.
func TestMain(m *testing.M) {
	all.IntegrationMain(m, all.KafkaName)
}

// TODO (silvano): add test that's driven by a seqtest.Generator
// https://github.com/cockroachdb/replicator/issues/789

// TestKafka verifies that we can process simple messages from Kafka.
// The kafka messages are generated by a CockroachDB changefeed in JSON format.
func TestKafka(t *testing.T) {

	t.Run("immediate", func(t *testing.T) { testIntegration(t, &fixtureConfig{immediate: true}) })
	t.Run("immediate chaos", func(t *testing.T) { testIntegration(t, &fixtureConfig{chaos: true, immediate: true}) })
	t.Run("immediate script", func(t *testing.T) { testIntegration(t, &fixtureConfig{script: true, immediate: true}) })

	t.Run("consistent", func(t *testing.T) { testIntegration(t, &fixtureConfig{}) })
	t.Run("consistent chaos", func(t *testing.T) { testIntegration(t, &fixtureConfig{chaos: true}) })
	t.Run("consistent script", func(t *testing.T) { testIntegration(t, &fixtureConfig{script: true}) })
}

func testIntegration(t *testing.T, fc *fixtureConfig) {
	log.SetLevel(log.DebugLevel)
	a := assert.New(t)
	r := require.New(t)
	var stopped <-chan struct{}
	defer func() {
		if stopped != nil {
			<-stopped
		}
	}()

	// Create a basic fixture to represent a source database.
	sourceFixture, err := base.NewFixture(t)
	r.NoError(err)

	ctx := sourceFixture.Context

	// Create a basic destination database connection.
	destFixture, err := base.NewFixture(t)
	r.NoError(err)

	targetDB := destFixture.TargetSchema.Schema()
	targetPool := destFixture.TargetPool

	// Set up source and target tables.
	source, err := sourceFixture.CreateSourceTable(ctx, "CREATE TABLE %s (pk INT PRIMARY KEY, v STRING)")
	r.NoError(err)

	// Since we're creating the target table without using the helper
	// CreateTable(), we need to manually refresh the target's Watcher.
	target := ident.NewTable(targetDB, source.Name().Table())
	targetCol := "v"
	if fc.script {
		targetCol = "v_mapped"
	}
	_, err = targetPool.ExecContext(ctx, fmt.Sprintf("CREATE TABLE %s (pk INT PRIMARY KEY, %s VARCHAR(2048))", target, targetCol))
	r.NoError(err)

	serverCfg, err := getConfig(destFixture, fc, []string{target.Raw()}, target)
	r.NoError(err)
	timeoutCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()
	connCtx := stopper.WithContext(timeoutCtx)
	kafka, err := Start(connCtx, serverCfg)
	r.NoError(err)

	watcher := kafka.Conn.conveyor.Watcher()
	r.NoError(watcher.Refresh(ctx, targetPool))

	createStmt := "CREATE CHANGEFEED FOR TABLE %s INTO" +
		fmt.Sprintf("'kafka://%s?topic_prefix=%s.'", broker, targetDB.Raw()) +
		" WITH updated,diff,resolved='1s',min_checkpoint_frequency='1s'"

	row := sourceFixture.SourcePool.QueryRowContext(ctx, fmt.Sprintf(createStmt, source.Name()))
	var id string
	r.NoError(row.Scan(&id))

	defer func() {
		cancelStmt := fmt.Sprintf("CANCEL JOB  %s", id)
		log.Info(cancelStmt)
		_, err := sourceFixture.SourcePool.ExecContext(ctx, cancelStmt)
		r.NoError(err)
	}()
	checkStatus := fmt.Sprintf(`
	SELECT running_status FROM [show changefeed jobs] 
	WHERE job_id = %s and running_status like '%s'`,
		id, "%resolved%")

	for {
		log.Infof("waiting for changefeed %s", target)
		row = sourceFixture.SourcePool.QueryRowContext(ctx, checkStatus)
		var status string
		err := row.Scan(&status)
		if errors.Is(err, sql.ErrNoRows) {
			time.Sleep(100 * time.Millisecond)
			continue
		}
		r.NoError(err)
		log.Infof("changefeed status %s", status)
		break
	}
	// Add base data to the source table.
	r.NoError(source.Exec(ctx, "INSERT INTO %s (pk, v) VALUES (1, 'one')"))
	ct, err := source.RowCount(ctx)
	r.NoError(err)
	a.Equal(1, ct)

	log.Infof("waiting for backfill %s", target)
	// Wait for the backfilled value.
	for {
		ct, err := base.GetRowCount(ctx, targetPool, target)
		r.NoError(err)
		if ct >= 1 {
			break
		}
		time.Sleep(time.Second)
	}

	// Update the first value
	r.NoError(source.Exec(ctx, "UPSERT INTO %s (pk, v) VALUES (1, 'updated')"))

	// Insert an additional value
	r.NoError(source.Exec(ctx, "INSERT INTO %s (pk, v) VALUES (2, 'two')"))
	ct, err = source.RowCount(ctx)
	r.NoError(err)
	a.Equal(2, ct)

	log.Infof("waiting for the insert %s", target)
	// Wait for the streamed value.
	for {
		ct, err := base.GetRowCount(ctx, targetPool, target)
		r.NoError(err)
		if ct >= 2 {
			break
		}
		time.Sleep(100 * time.Millisecond)
	}

	log.Infof("waiting for update %s", target)
	// Also wait to see that the update was applied.
	for {
		var val string
		r.NoError(targetPool.QueryRowContext(ctx,
			fmt.Sprintf("SELECT %s FROM %s WHERE pk = 1", targetCol, target),
		).Scan(&val))
		if val == "updated" {
			break
		}
		time.Sleep(100 * time.Millisecond)
	}

	r.NoError(source.Exec(ctx, "DELETE FROM %s WHERE PK = 2"))
	ct, err = source.RowCount(ctx)
	r.NoError(err)
	a.Equal(1, ct)

	log.Infof("waiting for delete %s", target)
	// Wait for the streamed value.
	for {
		ct, err := base.GetRowCount(ctx, targetPool, target)
		r.NoError(err)
		if ct == 1 {
			break
		}
		time.Sleep(100 * time.Millisecond)
	}

	connCtx.Stop(time.Second)
	r.NoError(connCtx.Wait())
	metrics, err := prometheus.DefaultGatherer.Gather()
	a.NoError(err)
	log.WithField("metrics", metrics).Trace()
	sinktest.CheckDiagnostics(ctx, t, kafka.Diagnostics)

}

// changefeedPartitioner forces a message without a key (e.g. the resolved timestamp)
// on a given partition.
type changefeedPartitioner struct {
	hash sarama.Partitioner
}

var _ sarama.Partitioner = &changefeedPartitioner{}
var _ sarama.PartitionerConstructor = newChangefeedPartitioner

func newChangefeedPartitioner(topic string) sarama.Partitioner {
	return &changefeedPartitioner{
		hash: sarama.NewCustomHashPartitioner(fnv.New32a)(topic),
	}
}

func (p *changefeedPartitioner) RequiresConsistency() bool { return true }
func (p *changefeedPartitioner) Partition(
	message *sarama.ProducerMessage, numPartitions int32,
) (int32, error) {
	if message.Key == nil {
		return message.Partition, nil
	}
	return p.hash.Partition(message, numPartitions)
}

func TestWorkload(t *testing.T) {
	t.Run("consistent", func(t *testing.T) { testWorkload(t, &fixtureConfig{}) })
	t.Run("consistent chaos", func(t *testing.T) { testWorkload(t, &fixtureConfig{chaos: true}) })

	t.Run("immediate", func(t *testing.T) { testWorkload(t, &fixtureConfig{immediate: true}) })
	t.Run("immediate chaos", func(t *testing.T) { testWorkload(t, &fixtureConfig{chaos: true, immediate: true}) })
}

func testWorkload(t *testing.T, fc *fixtureConfig) {
	log.SetLevel(log.DebugLevel)
	r := require.New(t)

	fixture, err := all.NewFixture(t, time.Minute)
	r.NoError(err)
	ctx := fixture.Context
	workload, _, err := fixture.NewWorkload(ctx,
		&all.WorkloadConfig{
			// Don't create foreign keys references in immediate mode
			DisableFK:      fc.immediate,
			DisableStaging: true,
		})
	r.NoError(err)
	topics := []string{
		workload.Parent.Name().Raw(),
		workload.Child.Name().Raw(),
	}
	serverCfg, err := getConfig(fixture.Fixture, fc, topics,
		workload.Parent.Name())
	r.NoError(err)

	producer, err := newProducer(serverCfg.Brokers, topics)
	r.NoError(err)
	defer func() {
		if err := producer.cleanup(); err != nil {
			log.Errorf("error closing producer %v", err)
		}
	}()
	timeoutCtx, cancel := context.WithTimeout(ctx, 30*time.Second)
	defer cancel()
	connCtx := stopper.WithContext(timeoutCtx)
	conn, err := Start(connCtx, serverCfg)
	r.NoError(err)
	stats := conn.Conn.conveyor.(interface {
		Stat() *notify.Var[sequencer.Stat]
	}).Stat()

	var clock hlc.Clock
	r.NoError(producer.writeResolved(clock.Now()))
	for iter := 1; iter <= maxIterations; iter++ {
		batch := &types.MultiBatch{}
		size := rand.IntN(maxBatchSize) + 1
		for i := 0; i < size; i++ {
			workload.GenerateInto(batch, clock.Now())
		}
		r.NoError(producer.writeBatch(batch))
		// Write a resolved timestamp file once in a while.
		// Ensure that we have resolved file at the end.
		if isPrime(iter) || iter == maxIterations {
			r.NoError(producer.writeResolved(clock.Now()))
		}
	}
	log.Info("waiting for rows")
	// Waiting for the rows to show in the target database.
	r.NoError(workload.WaitForCatchUp(ctx, stats))
	ticker := time.NewTicker(time.Second)
	defer ticker.Stop()
	for {
		parent, err := workload.Checker.StageCounter(workload.Parent.Name(),
			hlc.RangeIncluding(hlc.Zero(), clock.Last()))
		r.NoError(err)
		child, err := workload.Checker.StageCounter(workload.Child.Name(),
			hlc.RangeIncluding(hlc.Zero(), clock.Last()))
		r.NoError(err)
		log.Infof("staging database content parent rows: %d, child rows: %d", parent, child)
		if parent == 0 && child == 0 {
			break
		}
		select {
		case <-timeoutCtx.Done():
			r.Fail("timed out waiting for staging to be empty")
		case <-ticker.C:
		}
	}
	// Verify that the target database has all the data.
	workload.CheckConsistent(ctx, t)
	connCtx.Stop(time.Second)
	r.NoError(connCtx.Wait())
}

type kafkaProducer struct {
	producer sarama.SyncProducer
	topics   []string
}

func newProducer(brokers []string, topics []string) (*kafkaProducer, error) {
	config := sarama.NewConfig()
	config.Producer.Partitioner = newChangefeedPartitioner
	config.Producer.Return.Successes = true
	producer, err := sarama.NewSyncProducer(brokers, config)
	if err != nil {
		return nil, err
	}
	admin, err := sarama.NewClusterAdmin(brokers, config)
	if err != nil {
		return nil, err
	}
	defer func() { _ = admin.Close() }()
	for _, topic := range topics {
		err = admin.CreateTopic(topic, &sarama.TopicDetail{
			NumPartitions:     numPartitions,
			ReplicationFactor: 1,
		}, false)
		if err != nil {
			return nil, err
		}
	}
	return &kafkaProducer{
		producer: producer,
		topics:   topics,
	}, err
}

func (p *kafkaProducer) cleanup() error {
	return p.producer.Close()
}

func (p *kafkaProducer) sendMessage(
	topic string, partition int32, key json.RawMessage, message any,
) error {
	out, err := json.Marshal(message)
	if err != nil {
		return err
	}

	msg := &sarama.ProducerMessage{
		Topic:     topic,
		Partition: partition,
		Value:     sarama.ByteEncoder(out),
	}
	if key != nil {
		msg.Key = sarama.ByteEncoder(key)
	}
	sent, _, err := p.producer.SendMessage(msg)
	if key == nil && sent != partition {
		return errors.Errorf("invalid partition %d (%d)", sent, partition)
	}
	return err
}
func (p *kafkaProducer) writeBatch(batch *types.MultiBatch) error {
	type payload struct {
		After   json.RawMessage `json:"after"`
		Before  json.RawMessage `json:"before"`
		Key     json.RawMessage `json:"key"`
		Updated string          `json:"updated"`
	}
	for _, b := range batch.Data {
		for t := range b.Data.Values() {
			for _, v := range t.Data {
				msg := &payload{
					After:   v.Data,
					Before:  v.Before,
					Key:     v.Key,
					Updated: v.Time.String(),
				}
				// Changfeeds encode a deletion as an absence of an after block.
				if v.IsDelete() {
					msg.After = nil
				}
				err := p.sendMessage(t.Table.Raw(), -1, v.Key, msg)
				if err != nil {
					return err
				}
			}
		}
	}
	return nil
}

func (p *kafkaProducer) writeResolved(resolved hlc.Time) error {
	type resolvedPayload struct {
		Resolved string `json:"resolved"`
	}
	r := resolvedPayload{
		Resolved: resolved.String(),
	}
	for _, topic := range p.topics {
		var i int32
		for ; i < numPartitions; i++ {
			err := p.sendMessage(topic, i, nil, r)
			if err != nil {
				return err
			}
		}
	}
	return nil
}

func isPrime(i int) bool {
	return big.NewInt(int64(i)).ProbablyPrime(0)
}

// getConfig is a helper function to create a configuration for the connector
func getConfig(
	fixture *base.Fixture, fc *fixtureConfig, topics []string, tgt ident.Table,
) (*Config, error) {
	dbName := fixture.TargetSchema.Schema()
	crdbPool := fixture.TargetPool
	config := &Config{
		Staging: sinkprod.StagingConfig{
			Schema: fixture.StagingDB.Schema(),
		},
		Target: sinkprod.TargetConfig{
			CommonConfig: sinkprod.CommonConfig{
				Conn: crdbPool.ConnectionString,
			},
			ApplyTimeout: 2 * time.Minute, // Increase to make using the debugger easier.
		},
		TargetSchema: dbName,
		Conveyor: conveyor.Config{
			Immediate: fc.immediate,
		},
		BatchSize:        100,
		Brokers:          []string{broker},
		Group:            dbName.Raw(),
		ResolvedInterval: time.Second,
		Strategy:         "sticky",
		Topics:           topics,
	}
	if fc.chaos {
		config.Sequencer.Chaos = 2
	}
	if fc.script {
		config.Script = script.Config{
			FS:       scripttest.ScriptFSFor(tgt),
			MainPath: "/testdata/logical_test.ts",
		}
	}
	return config, config.Preflight(fixture.Context)
}
